  0%|                                                                                  | 0/280 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 128, in <module>
  File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 122, in train
    save_name = "checkpoint-best" if training_args.load_best_model_at_end else "checkpoint-final"
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2326, in train
    return inner_training_loop(
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2621, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 5608, in get_batch_samples
    batch_samples.append(next(epoch_iterator))
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/accelerate/data_loader.py", line 563, in __iter__
    current_batch = next(dataloader_iter)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 757, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/data/data_collator.py", line 683, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/data/data_collator.py", line 67, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3416, in pad
    padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2835, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 128, in <module>
[rank0]:   File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 122, in train
[rank0]:     save_name = "checkpoint-best" if training_args.load_best_model_at_end else "checkpoint-final"
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2326, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2621, in _inner_training_loop
[rank0]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 5608, in get_batch_samples
[rank0]:     batch_samples.append(next(epoch_iterator))
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/accelerate/data_loader.py", line 563, in __iter__
[rank0]:     current_batch = next(dataloader_iter)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 757, in _next_data
[rank0]:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/data/data_collator.py", line 683, in __call__
[rank0]:     batch = pad_without_fast_tokenizer_warning(
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/data/data_collator.py", line 67, in pad_without_fast_tokenizer_warning
[rank0]:     padded = tokenizer.pad(*pad_args, **pad_kwargs)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3416, in pad
[rank0]:     padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2835, in _get_padding_truncation_strategies
[rank0]:     raise ValueError(
[rank0]: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
