  0%|                                                                                  | 0/280 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 134, in <module>
  File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 128, in train
    save_name = "checkpoint-best" if training_args.load_best_model_at_end else "checkpoint-final"
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2326, in train
    return inner_training_loop(
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2688, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 4087, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
    self.engine.step()
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2249, in step
    self._take_model_step(lr_kwargs)
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2152, in _take_model_step
    self.optimizer.step()
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1887, in step
    single_grad_partition = self.flatten_dense_tensors_aligned(
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 931, in flatten_dense_tensors_aligned
    return self.flatten(align_dense_tensors(tensor_list, alignment))
  File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/_utils.py", line 519, in _flatten_dense_tensors
    return torch._C._nn.flatten_dense_tensors(tensors)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.96 GiB. GPU 0 has a total capacity of 79.32 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.63 GiB memory in use. Of the allocated memory 59.85 GiB is allocated by PyTorch, and 14.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 134, in <module>
[rank0]:   File "/c23030/ckj/code/vlm/jailbreaking/sft_llm_cot.py", line 128, in train
[rank0]:     save_name = "checkpoint-best" if training_args.load_best_model_at_end else "checkpoint-final"
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2326, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 2688, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py", line 4087, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
[rank0]:     self.engine.step()
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2249, in step
[rank0]:     self._take_model_step(lr_kwargs)
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2152, in _take_model_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1887, in step
[rank0]:     single_grad_partition = self.flatten_dense_tensors_aligned(
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 931, in flatten_dense_tensors_aligned
[rank0]:     return self.flatten(align_dense_tensors(tensor_list, alignment))
[rank0]:   File "/c23030/ckj/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/_utils.py", line 519, in _flatten_dense_tensors
[rank0]:     return torch._C._nn.flatten_dense_tensors(tensors)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.96 GiB. GPU 0 has a total capacity of 79.32 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.63 GiB memory in use. Of the allocated memory 59.85 GiB is allocated by PyTorch, and 14.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
