# ReSAM Framework (Anonymous)

This repository contains the anonymized implementation of the proposed framework for data generation, model training, and evaluation used in our paper.

> This repository is anonymized for double-blind review.

---

## Project Structure

### 1. Data Generation

Scripts for constructing training and evaluation data, including multiple attack and reflection-based pipelines.

- `generate.py`  
  Test script for generating single-sample data for the initial version of the framework.  
  Includes demos of multiple attack methods.

- `generate_reflection_benign_data.py`  
  Reads both `item` and `benign_item` from a given `INPUT_PATH` as initial data.  
  For each input, it queries an LLM twice to generate:
  - reflection  
  - exploration  
  - continuation  

  The generated components are then combined into a complete reflective response.

- `generate_reflection_data.py`  
  Main pipeline for reflection data generation, including the following components:

  - `DRA/`  
  - `src/attack/`  
  - `test_openai.py`  
  - `clean_data.py`  

---

### 2. Model Training

Scripts for supervised fine-tuning with chain-of-thought style data.

- `sft.bash`  
  Training script containing all hyperparameters and configuration settings.

- `sft_llm_cot.py`  
  Core training implementation, including:
  - dataset loading  
  - model initialization  
  - training loop  
  - checkpoint saving  

---

### 3. Evaluation

- `eval_llm.py`  
  Evaluates model performance on a specified dataset.  
  The model outputs are saved to corresponding files under: results/model_answer/


- `eval_json.py`  
Evaluates the harmfulness of model responses.

This script:
- takes the path to model outputs as input,  
- reformats data into `{'question': xxx}` format,  
- applies multiple harmfulness detection methods, including HarmBench-style evaluation.

The HarmBench evaluation is implemented by prompting an LLM to perform binary harmfulness classification.

---

### 4. Result Files

The `results/` directory contains:

- `DRA_processed/`  
Successful adversarial samples generated by the DRA attack against `llama3.1-8b-instruct`.

- `model_answer/`  
Model responses on different datasets after training.

Example files:
- `strongreject.json`: raw model responses on the StrongReject dataset  
- `strongreject_check.json`: harmfulness evaluation results after:
  - HarmBench classification  
  - three string-matching based checks  

In the processed files:
- `True` indicates the response contains harmful content  
- `False` indicates the response is safe  

---

## TODO

- Investigate what types of generalization capabilities emerge from the framework.  
- Analyze how internal model parameters and representations change after training.
